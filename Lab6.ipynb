{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw4Ga6VT6dn2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wstęp\n",
    "\n",
    "![A photograph of a recommender system made from a matric.](stable_diffusion_image.jpeg \"Stable Diffusion image: A photograph of a recommender system made from a matric\")\n",
    "\n",
    "\n",
    "Celem laboratorium jest poznanie podstaw systemów rekomendacyjnych. Zapoznasz się na nim z następującymi tematami:\n",
    "* budową systemów rekomendacyjnych typu collaborative filtering (CF), w szczególności z:\n",
    "  * macierzą interakcji użytkownik-przedmiot (user-item matrix)\n",
    "  * pojęciem biasu użytkownika i przedmiotu\n",
    "  * analizą zbiorów danych do CF\n",
    "  * metrykami jakości dla systemów rekomendacyjnych\n",
    "* algorytmami globalnej rekomendacji:\n",
    "  * metodami podstawowymi (baselines)\n",
    "  * metodami bayesowskimi (Bayesian average)\n",
    "* algorytmami personalizowanej rekomendacji typu CF, w szczególności z:\n",
    "  * metodą najbliższych sąsiadów (neighborhood-based) typu user-based oraz item-based\n",
    "  * rozkładem macierzowym (matrix factorization) typu MF oraz FunkSVD\n",
    "\n",
    "Jak zwykle, możesz albo korzystać z Google Colab, albo z własnego komputera. W obu przypadkach trzeba doinstalować trochę bibliotek.\n",
    "\n",
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/apohllo/sztuczna-inteligencja/blob/master/lab6/Lab6.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "643x_mk-6dn3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Krótki wstęp teoretyczny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDPFRVa_6dn3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Systemy rekomendacyjne (recommender systems)** to dowolne metody mające rekomendować użytkownikom (users) pewne przedmioty (items). Korzysta z nich praktycznie każda większa firma: Netflix (filmy - \"Top picks for you\"), Spotify (muzyka, \"Recommended for playlist\"), Amazon (sklep - \"frequently bought together\") etc. Mają niesamowicie praktyczne zastosowanie i są jednym z najwcześniej oraz najczęściej wdrażanych metod uczenia maszynowego.\n",
    "\n",
    "Jest to bardzo szeroka dziedzina, o bardzo różnorodnych podejściach. W szczególności można wyróżnić grupy: \n",
    "1. **Collaborative filtering (CF)** - oparte o historię interakcji użytkowników z przedmiotami, czyli zwykle o historię ocen. Stąd pochodzą np. rekomendacje \"użytkownicy podobni do ciebie oglądali także X\", gdzie podobieństwo mierzy się na podstawie tego, jak bardzo podobne mieliśmy w przeszłości oceny do innych użytkowników. Co ważne, takie podejście nie wymaga żadnej inżynierii cech, a jedynie zapamiętania historii ocen / transakcji / interakcji!\n",
    "2. **Content-based (CB)** - dużo bardziej podobne do klasycznego ML, tworzymy wektory cech dla przedmiotów, użytkowników i wykorzystujemy je w klasyfikacji (np. rekomendować lub nie) lub regresji (np. liczba gwiazdek).\n",
    "3. **Algorytmy hybrydowe** - łączące podejścia CF i CB podczas nauki. Są zazwyczaj bardziej złożone i wymagają odpowiednio dużych zbiorów danych.\n",
    "\n",
    "Dodatkowo możemy podzielić problemy rekomendacji na dwa rodzaje, w zależności od tego, czym są nasze **oceny (ratings)**:\n",
    "1. **Explicit feedback** - kiedy użytkownicy jawnie podają oceny, np. ocena hotelu w skali 1-10, liczba gwiadek dla przedmiotu. Wymaga to większej proaktywności użytkowników, więc potencjalnie możemy mieć mniej danych, ale są często bardziej precyzyjne. Są też typowo prostsze teoretycznie (matematycznie), bo mają znany z góry, ograniczony zakres możliwych wartości.\n",
    "2. **Implicit feedback** - kiedy jakość przedmiotu wyznaczają akcje użytkowników, np. liczba kliknięć, liczba udostępnień. Takie informacje można gromadzić automatycznie i bardzo łatwo, ale mogą być mało precyzyjne (np. przypadkowe kliknięcia, boty). Algorytmy dla takich problemów są też cięższe do zaprojektowania, bo mamy tylko wartości nieujemne i typowo nieograniczone z góry.\n",
    "\n",
    "Same rekomendacje mogą być dwojakiego rodzaju:\n",
    "1. **Globalne (global)** - biorą pod uwagę ogólne cechy przedmiotu i są oceniane dla całej społeczności, a nie dla konkretnego użytkownika. Korzystają z nich typowo strony z wiadomościami, żeby ułożyć kolejność postów na stronie, np. HackerNews, Reddit. Przydają się też, gdy nie mamy dość informacji o użytkowniku, aby dokonać personalizacji.\n",
    "2. **Personalizowane (personalized)** - zasadnicze zastosowanie systemów rekomendacyjnych, w którym \"profilujemy\" użytkownika lub przedmiot, tak, aby nauczyć sie relacji między nimi i sugerować to, co konkretną osobę może interesować.\n",
    "\n",
    "Na tym laboratorium skupimy się na systemach typu collaborative filtering, bo są:\n",
    "1. Ciekawsze i bardziej unikatowe na tle tych algorytmów, które już poznaliśmy.\n",
    "2. Często o wiele łatwiejsze w praktycznej implementacji, gdyż nie wymagają inżynierii cech.\n",
    "3. Bardzo szybkie i skalowalne.\n",
    "4. Zazwyczaj lepsze pod względem wyników od systemów content-based.\n",
    "\n",
    "Skupimy się na systemach typu explicit ranking, bo są nieco prostsze i popularniejsze. Poznamy za to i systemy globalne, i personalizowane.\n",
    "\n",
    "Czemu więc korzystać z innego podejścia niż CF? O tym przekonasz się w późniejszej części laboratorium :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNC00UuE6dn4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Biblioteki do systemów rekomendacyjnych\n",
    "\n",
    "Do systemów rekomendacyjnych nie istnieje jedna standardowa, powszechnie przyjęta biblioteka, taka jak Scikit-learn. Jest to bowiem zbyt rozległa dziedzina, oparta o bardzo różnorodne podejścia i rozwiązania, aby dało się ją zamknąć w jednej bibliotece ze spójnym interfejsem. Można jednak wyróżnić zbiór najpopularniejszych bibliotek. Co ważne, praktyczne systemy implementuje się jednak często od zera, dla konkretnego problemu.\n",
    "\n",
    "1. [Surprise](https://surpriselib.com/) - od niedawna `scikit-surprise` ze względu na implementację interfejsów ze Scikit-learn'a. Implementuje algorytmy typu explicit rating collaborative filtering.\n",
    "2. [Implicit](https://benfred.github.io/implicit/) - podobna do Surprise, implementuje algorytmy typu implicit rating collaborative filtering.\n",
    "3. [LibRecommender](https://github.com/massquantity/LibRecommender) - rozbudowana biblioteka, implementująca różne podejścia: collaborative filtering, feature-based, oraz hybrydowe. Zawiera algorytmy pisane od zera, w TensorFlow (niestety v1) oraz w PyTorchu, na podstawie szeregu artykułów naukowych. Ma jednak dość specyficzny, niekoniecznie intuicyjny interfejs.\n",
    "4. [Spark MLlib](https://spark.apache.org/docs/latest/ml-collaborative-filtering.html) - de facto standard w pracy z wielkimi zbiorami danych, częstymi w systemach rekomendacyjnych. Implementuje explicit oraz implicit collaborative filtering.\n",
    "5. [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) - de facto standard dla grafowych sieci neuronowych (Graph Neural Networks, GNNs), które są najnowszym trendem w systemach rekomendacyjnych opartych o grafy (graph-based recommender systems).\n",
    "\n",
    "Dodatkowo dla podejścia content-based (opisane, ale nie implementowane w tym laboratorium) można użyć dowolnej biblioteki do uczenia nadzorowanego, typowo Scikit-learn lub Spark MLlib.\n",
    "\n",
    "Na tym laboratorium wykorzystamy `Surprise` ze względu na prostotę użycia. Dodatkowo użyjemy `recmetrics`, aby obliczyć metryki specyficzne dla systemów rekomendacyjnych, których nie implementuje Scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aMdfhlN56dn4",
    "outputId": "2b87fa23-1745-4fe0-8d19-9c5b093596db",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install scikit-surprise recmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETVT2G8E6dn4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Ściąganie, ładowanie i eksploracja danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aijq-Si86dn4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Na początek ściągniemy nasz zbiór danych. Będziemy pracować na zbiorze MovieLens-100k, czyli zbiorze 100 tysięcy ocen filmów. Strona MovieLens udostępnia wiele rozmiarów tego zbioru danych, a ten będzie odpowiednio szybki na potrzeby edukacyjne. W praktyce wykorzystuje się zbiory rozmiaru co najmniej takiego, jak MovieLens-1M (zbiór miliona ocen).\n",
    "\n",
    "Opis plików można znaleźć w [readme](https://files.grouplens.org/datasets/movielens/ml-100k-README.txt). Najważniejsze fragmenty:\n",
    "```\n",
    "u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.\n",
    "              Each user has rated at least 20 movies.  Users and items are\n",
    "              numbered consecutively from 1.  The data is randomly\n",
    "              ordered. This is a tab separated list of \n",
    "\t         user id | item id | rating | timestamp. \n",
    "              The time stamps are unix seconds since 1/1/1970 UTC   \n",
    "```\n",
    "\n",
    "Zbiór co prawda ma już przygotowany podział do 5-krotnej walidacji skrośnej (pliki `u1.base`, `u1.test` etc.), ale my wykonamy ten podział sami. Gotowych podziałów używa się w pracach naukowych, aby móc porównywać wyniki różnych algorytmów na dokładnie tych samych zbiorach treningowych i testowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZT_YQsEi6dn5",
    "outputId": "e8e17497-f305-4dda-d570-306819b25816",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!wget -N https://files.grouplens.org/datasets/movielens/ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s0J7s2ds6dn5",
    "outputId": "cba9fd99-ef8e-492d-e539-f084295e8281",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!unzip -n ml-100k.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3WqtvaO76dn5",
    "outputId": "38e93f05-72f7-4c47-f660-fec922ccf032",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\n",
    "    os.path.join(\"ml-100k\", \"u.data\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"user_id\", \"item_id\", \"rating\", \"timestamp\"],\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MgWaFF1e6dn5",
    "outputId": "09a6f0b7-2871-49da-e5f3-3d5013bb83a7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Number of reviews: {len(df)}\")\n",
    "print(f\"Ratings range: {df.rating.min(), df.rating.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsEBbAx-6dn5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tabela w formacie jak powyżej to de facto słownik `(user_id, item_id) -> rating`. Jest zatem idealna do podejścia collaborative filtering, w którym dla **użytkowników (users)** mamy ich **oceny (ratings)** wybranych **przedmiotów (items)**. Tutaj oczywiście przedmiotami są filmy. Można by zatem z takich danych zbudować **macierz ocen (ratings matrix)**, w której wiersze byłyby użytkownikami, kolumny przedmiotami, a komórki zawierałyby oceny. \n",
    "\n",
    "\n",
    "W przyszłości chcemy zatem **przewidywać wartości brakujące** macierzy ocen. Mamy tu zatem poniekąd problem regresji - chcemy dostać wartość ciągłą, np. na ile użytkownik oceniłby film, którego jeszcze nie widział. Późniejsza rekomendacja to po prostu wybranie najwyższych predykcji i zaproponowanie tych właśnie przedmiotów. Nazywa się to czasem problemem **uzupełnienia macierzy (matrix completion)**.\n",
    "\n",
    "W praktyce **nigdy** nie należy budować macierzy ocen explicite. Zwyczajnie nie zmieściłaby się ona do pamięci dla zbiorów o prawdziwym rozmiarze, kiedy mamy setki tysięcy użytkowników i przedmiotów. Dodatkowo zwyczajnie nie ma to sensu, bo nasze macierze prawie zawsze są **rzadkie (sparse)**, tzn. mają wypełnioną tylko nieznaczną liczbę pól. Reszta jest nieznana - w końcu pojedynczy człowiek obejrzy tylko niewielką część wszystkich filmów z Netflixa, nie mówiąc już o wystawieniu im ocen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZH-Jk6EZ6dn5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Zadanie 1 (0.5 punktu)**\n",
    "\n",
    "Oblicz gęstość (density) macierzy ocen dla naszego zbioru danych. Jest to liczba ocen, podzielona przez rozmiar macierzy ocen (liczba użytkowników * liczba przedmiotów). Wynik przedstaw w procentach, zaokrąglony do 4 miejsc po przecinku. Pamiętaj, żeby uwzględnić tylko unikatowych użytkowników i przedmioty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-01-03T12:29:18.510593Z",
     "start_time": "2023-01-03T12:29:18.506886Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1127zu4s6dn5",
    "outputId": "806ba368-0bc4-484b-f271-a2ce61059eb5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(f'{df[\"rating\"].size / df[\"user_id\"].nunique() / df[\"item_id\"].nunique() * 100:.4f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QtoWIHfC6dn5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "W praktyce często zbiory mają gęstość rzędu 1% lub mniejszą. Jest to też pozytywne - w końcu to dzięki temu mamy komu robić rekomendacje (i czego).\n",
    "\n",
    "Warto zauważyć, że nasz zbiór zawiera tylko tych użytkowników, którzy ocenili przynajmniej 20 filmów, a zatem wiemy o każdym z nich całkiem sporo. Unika to problemu **zimnego startu (cold start)**, w którym nic nie wiemy o nowych użytkownikach i/lub filmach. W prawdziwych systemach jest to jednak duże wyzwanie. Można sobie z nim radzić na kilka sposobów:\n",
    "- rekomendować najpopularniejsze przedmioty,\n",
    "- rekomendować przedmioty o najwyższych ocenach,\n",
    "- użyć globalnego (niepersonalizowanego) systemu rekomendacyjnego, np. przewidywanie średniej dla przedmiotu,\n",
    "- używać systemu content-based, bo radzą sobie dobrze przy małej liczbie interakcji,\n",
    "- poprosić użytkownika przy pierwszym logowaniu o podanie pierwszych preferencji (nie zawsze jest to możliwe).\n",
    "\n",
    "W związku z problemem zimnego startu systemy rekomendacyjne zwykle są (co najmniej) dwuetapowe i mają osobny algorytm dla nowych użytkowników/przedmiotów oraz osobny dla tych, o których już coś wiemy więcej i możemy dokonywać personalizacji.\n",
    "\n",
    "Zbadajmy teraz rozkład popularności poszczególnych przedmiotów w naszym zbiorze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WesDuodh6dn5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Zadanie 2 (0.5 punktu)**\n",
    "\n",
    "Narysuj wykres popularności (liczby ocen) dla poszczególnych przedmiotów. Wykorzystaj do tego funkcję `long_tail_plot()` z biblioteki `recmetrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jp3MBcu7IE3m",
    "outputId": "344cec00-359c-4cee-c41b-7930ba58fdc2"
   },
   "outputs": [],
   "source": [
    "pip install matplotlib==3.1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 530
    },
    "id": "7nFa4IMh6dn5",
    "outputId": "81756b9b-650f-4742-fb77-db56fa7304b3",
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from recmetrics import long_tail_plot\n",
    "\n",
    "fig = plt.figure(figsize=(15, 7))\n",
    "long_tail_plot(df=df, \n",
    "             item_id_column=\"item_id\", \n",
    "             interaction_type=\"movie ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5A9vUINn6dn6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Typowo niewielka liczba przedmiotów odpowiada za większość ocen. Są to rzeczy bardzo znane i popularne, napędzane efektem kuli śnieżnej. Przykładowo, \"Titanic\" ogląda i ocenia bardzo znaczna liczba użytkowników, przez sam fakt, jak bardzo znany jest ten film. My jesteśmy zwykle zainteresowani **długim ogonem (long tail)** naszego rozkładu popularności, czyli zwiększeniem popularności tych przedmiotów, które są mniej znane, a które możemy zaoferować użytkownikom, np. nowa muzyka do odkrycia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxeB7gTA6dn6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Eksploracja danych - podsumowanie**\n",
    "\n",
    "1. W systemach typu collaborative filtering operujemy na macierzy ocen, gdzie wierszami są użytkownicy, kolumnami oceny, a w komórkach znajdują się oceny.\n",
    "2. Macierz ocen jest zwykle bardzo rzadka.\n",
    "3. Kiedy niewiele wiemy o użytkowniku lub przedmiocie, to mamy problem zimnego startu, z którym trzeba sobie w jakiś sposób poradzić.\n",
    "4. Często występuje zjawisko długiego ogona, czyli dominacji niewielkiej grupy bardzo popularnych przedmiotów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOBy9UEx6dn6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Walidacja modeli, prosty model bazowy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xC3-vtdP6dn6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Na początek, zanim zaczniemy budować nasze modele, trzeba wyodrębnić zbiór testowy. Mamy tutaj kilka możliwości. Po pierwsze, można zrobić to po prostu losowo, tak jak to robiliśmy do tej pory, i tak bardzo często się robi.\n",
    "\n",
    "Zbiór testowy ma jednak symulować przyszłe dane, przybliżać zdolność generalizacji modelu, a my mamy do dyspozycji znaczniki czasowe, z kiedy pochodzą dane oceny. Można by więc użyć **podziału czasowego (time split)**, czyli wyodrębnić najnowsze oceny do zbioru testowego, a konkretnie najnowsze oceny per użytkownik. Stanowi to bardzo dobrą symulację tego, jak w praktyce działa system.\n",
    "\n",
    "Powyższe podejścia mają jednak pewne ryzyko - może się zdarzyć, że tak wylosujemy zbiór testowy, że dla jakiegoś użytkownika 90% ocen jest w zbiorze testowym, więc spowodujemy u niego przypadkiem problem zimnego startu. Analogicznie może być przy podziale czasowym, kiedy jakiś nowy użytkownik był aktywny tylko niedawno i być może nawet wszystkie jego predykcje trafiłyby do zbioru testowego. Dlatego można stosować **podział per użytkownik**, wyodrębniając np. losowe 10% ocen każdego użytkownika jako zbiór testowy.\n",
    "\n",
    "Jak widać, jest tu nieco ciężej niż przy zwykłej klasyfikacji czy regresji. Dla uproszczenia wykorzystamy zwykły podział losowy. Implementacje innych metod można znaleźć np. w bibliotece LibRecommender.\n",
    "\n",
    "Surprise definiuje 2 ważne klasy: `Dataset` i `Trainset`. Ta pierwsza reprezentuje surowe dane, a druga wstępnie przetworzone dane do treningu lub testowania. Interfejs jest tutaj dość dziwny, ale w skrócie:\n",
    "- do zwykłych algorytmów idą `train_set` i `test_set`\n",
    "- do `GridSearchCV` idą `data_train` i `test_set`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNF2tQ2I6dn6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from surprise.dataset import Dataset, Reader\n",
    "\n",
    "reader = Reader(rating_scale=(df[\"rating\"].min(), df[\"rating\"].max()))\n",
    "dataset = Dataset.load_from_df(df[[\"user_id\", \"item_id\", \"rating\"]], reader=reader)\n",
    "\n",
    "ratings_train, ratings_test = train_test_split(\n",
    "    dataset.raw_ratings, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "data_train = deepcopy(dataset)\n",
    "data_train.raw_ratings = ratings_train\n",
    "\n",
    "train_set = data_train.build_full_trainset()\n",
    "test_set = data_train.construct_testset(ratings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJ5F4N-Y6dn6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Na początek zaimplementujemy model, który przewiduje po prostu wartość średnią dla każdego przedmiotu. Nie ma on żadnych hiperparametrów, więc nawet nie będziemy potrzebować zbioru walidacyjnego. Jest to bardzo dobry **model bazowy (baseline)** w systemach rekomendacyjnych.\n",
    "\n",
    "**Zadanie 3 (1 punkt)**\n",
    "\n",
    "Uzupełnij kod klasy `ItemAveragePredictor`, która przewiduje wartość średnią dla każdego przedmiotu. Może ci się tutaj przydać atrybut `ir` (item rating) klasy `Trainset` - [dokumentacja](https://surprise.readthedocs.io/en/stable/trainset.html), oraz [dokumentacja tworzenia własnych algorytmów](https://surprise.readthedocs.io/en/stable/building_custom_algo.html).\n",
    "\n",
    "Dobrym pomysłem będzie przechowywanie danych w postaci atrybutu będącego słownikiem w `.fit()`, żeby zapamiętać mapowanie `item_id` -> średnia ocena.\n",
    "\n",
    "Uwaga - zgodnie z konwencją ze Scikit-learn atrybuty, których wartości są obliczane (estymowane) na podstawie danych treningowych, są tworzone w metodzie `.fit()` i mają underscore `_` na końcu nazwy, np. `self.ratings_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNSoaLcX6dn6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from surprise import AlgoBase, PredictionImpossible\n",
    "\n",
    "\n",
    "class ItemAveragePredictor(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "        self.ratings_ = {}\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "      \n",
    "        for item_id, ratings in trainset.ir.items():\n",
    "          if (len(ratings) > 0):\n",
    "            avg_rating = 0\n",
    "            for _, r in ratings:\n",
    "              avg_rating += r\n",
    "            avg_rating /= len(ratings)\n",
    "            self.ratings_[item_id] = avg_rating\n",
    "          else:\n",
    "            self.ratings_[item_id] = trainset.global_mean\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible(\"User and/or item is unknown.\")\n",
    "\n",
    "        return self.ratings_[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rQqW8Lb56dn6",
    "outputId": "13807101-1036-4ca7-8718-2f16489e2464",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "algo = ItemAveragePredictor()\n",
    "algo.fit(train_set)\n",
    "pred_item_avg = algo.test(test_set)\n",
    "pred_item_avg[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIwzEhoQ6dn6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Musimy teraz ocenić jakość naszego algorytmu. Jako że mamy tu problem regresji, to naturalnym wyborem są **RMSE (root mean squared error)** oraz **MAE (Mean Absolute Error)**. Pokażą nam one, jak bardzo średnio nasz model myli się w przewidywaniu ratingu.\n",
    "\n",
    "RMSE to po prostu pierwiastek błędu średniokwadratowego (MSE). Ma taką samą wadę przy ewaluacji jak MSE - zwraca zbyt dużą uwagę na obserwacje odstające (outliers). Dzięki pierwiastkowaniu ma tę samą jednostkę, co oryginalne dane.\n",
    "$$\\large\n",
    "RMSE = \\sqrt{MSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N \\left( y_i - \\hat{y}_i \\right)^2}\n",
    "$$\n",
    "\n",
    "MAE to średnie odchylenie predykcji od wartości prawdziwej. Dzięki zastosowaniu wartości bezwzględnej zamiast kwadratu, jest miarą bardziej odporną na outliery i dlatego często wykorzystywaną przy ewaluacji. Ma naturalnie tę samą jednostkę, co mierzona wartość.\n",
    "$$\\large\n",
    "MAE = \\frac{1}{N} \\sum_{i=1}^N \\left| y_i - \\hat{y}_i \\right|\n",
    "$$\n",
    "\n",
    "Ze względu na to, że Surprise nie zwraca zwykłego wektora Numpy'a, tylko obiekty `Prediction`, trzeba użyć metryk z tej biblioteki. Zwykle nie stanowi to problemu, a dodatkowo mamy też do dyspozycji wszystko, co implementuje biblioteka recmetrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cLrCmDGY6dn6",
    "outputId": "b1ab32f2-0993-4012-9ebd-524b18584f60",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from surprise.accuracy import rmse, mae\n",
    "\n",
    "rmse(pred_item_avg, verbose=True)\n",
    "mae(pred_item_avg, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV4yj4Xo6dn6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wygląda na to, że nawet najprostszy model nie radzi sobie wcale tak źle. Ale są to tylko przewidywane wartości - zobaczmy faktyczne rekomendacje. W praktyce mamy ograniczone miejsce, np. mało kto popatrzy na więcej niż pierwsze 5-10 rekomendowanych filmów. W związku z tym nieważne nawet, co będzie dalej - liczy się dla nas **top k** predykcji.\n",
    "\n",
    "Zgromadzimy teraz faktyczne najlepsze oceny ze zbioru testowego dla każdego użytkownika, rekomendacje naszego systemu i zbierzemy je w jednen DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "TBwA1JvR6dn6",
    "outputId": "48a57bef-3234-4633-ec06-a15bec5ce76c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from surprise import Prediction\n",
    "from typing import List\n",
    "\n",
    "\n",
    "def get_user_recommendations(user_rec_items: pd.Series) -> List[int]:\n",
    "    return user_rec_items.sort_values(ascending=False).index.tolist()\n",
    "\n",
    "\n",
    "def get_recommendations(predictions: List[Prediction]) -> pd.DataFrame:\n",
    "    df_pred = pd.DataFrame(predictions)\n",
    "    df_pred = df_pred.drop(columns=\"details\")\n",
    "    df_pred.columns = [\"user_id\", \"item_id\", \"actual\", \"prediction\"]\n",
    "\n",
    "    df = (\n",
    "        df_pred.groupby(\"user_id\", as_index=False)[\"item_id\"]\n",
    "        .agg({\"actual\": (lambda x: list(x))})\n",
    "        .set_index(\"user_id\")\n",
    "    )\n",
    "\n",
    "    df_pivot = df_pred.pivot_table(\n",
    "        index=\"user_id\", columns=\"item_id\", values=\"prediction\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    df[\"recommendations\"] = [\n",
    "        get_user_recommendations(df_pivot.loc[user_id]) for user_id in df.index\n",
    "    ]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "rec_item_avg = get_recommendations(pred_item_avg)\n",
    "rec_item_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOjWdZON6dn7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Teraz kiedy można porównać faktyczne predykcje, patrząc np. na top 5, to nie wygląda to już tak dobrze, jak trzeba wybrać konkretne filmy. Do mierzenia jakości wśród top k predykcji służą metryki:\n",
    "- mean average precision at k (MAP@k)\n",
    "- mean average recall at k (MAR@k)\n",
    "- Fraction of Concordant Pairs (FCP)\n",
    "- Normalized Discounted Cumulative Gain (NDCG)\n",
    "\n",
    "Są one używane w systemach rekomendacyjnych, ale też w wyszukiwarkach i niektórych problemach NLP. Dla MAP i MAR dokładny opis, krok po kroku, możesz znaleźć [tutaj](https://sdsawtelle.github.io/blog/output/mean-average-precision-MAP-for-recommender-systems.html) i [tutaj](https://machinelearninginterview.com/topics/machine-learning/mapatk_evaluation_metric_for_ranking/). FCP doskonale opisuje [oryginalny artykuł](https://www.ijcai.org/Proceedings/13/Papers/449.pdf). [Tutaj](https://finisky.github.io/2019/04/24/ndcg/) krótki i treściwy artykuł o NDCG.\n",
    "\n",
    "### MAP@k\n",
    "\n",
    "Zdefiniujmy:\n",
    "- True Positive (TP) - przedmiot, który naprawdę jest w top k (*relevant*) i który nasz system zarekomendował w top k przedmiotów\n",
    "- False Positive (FP) - przedmiot, który nie jest w top k (*nonrelevant*), ale nasz model go zarekomendował w top k\n",
    "\n",
    "\"Precision at k\" to precyzja (precision), obliczona dla top k przedmiotów. Oznaczmy przez $r_k$ liczbę TP (*relevant items*) wśród top k przedmiotów.\n",
    "$$\\large\n",
    "P@k = \\frac{\\text{number of relevant items in top }k}{k} = \\frac{r_k}{k}\n",
    "$$\n",
    "\n",
    "\"Average P@k\" to po prostu P@k obliczone dla różnych $i=1,2,...,k$ i uśrednione. Taka agregacja bardzo penalizuje umieszczanie nieciekawych przedmiotów na wysokich miejscach, bo uwzględniamy tylko precyzję dla tych top k, gdzie prawidłowo zauważyliśmy TP.\n",
    "$$\\large\n",
    "AP@k = \\frac{1}{r_k} \\sum_{i=1}^{k} \\left( P@i \\text{ if i-th item is relevant} \\right)\n",
    "$$\n",
    "\n",
    "MAP@k to AP@k (average precision at k), uśrednione dla wszystkich $|U|$ użytkowników:\n",
    "$$\\large\n",
    "MAP@k = \\frac{1}{|U|} \\sum_{u=1}^{|U|}AP@k(u)\n",
    "$$\n",
    "\n",
    "Im niższe $k$, tym surowsi jesteśmy i tym niższe będą wyniki - nasz algorytm musi umieścić poprawne predykcje bardzo wysoko, aby uzyskać wartość niezerową. Typowo $k=5$ lub $k=10$. Zakres wartości MAP@k to $[0, 1]$.\n",
    "\n",
    "MAP@k przykłada bardzo dużą wagę do tego, żeby na pierwszych miejscach trafiły się jak najlepsze przedmioty. Jest zatem bardzo ważne, kiedy mamy mało miejsc do dyspozycji, np. przy rekomendacji filmów na głównej stronie (Netflix).\n",
    "\n",
    "### FCP\n",
    "\n",
    "FCP (Fraction of Concordant Pairs) jest rzadziej używaną, ale bardzo intuicyjną metryką. Ideą jest uogólnienie metryki AUROC (ROC AUC) na algorytmy rankujące, a więc m.in. systemy rekomendacyjne. Ma zakres wartości $[0, 1]$.\n",
    "\n",
    "Liczba zgodnych par (*concordant pairs*) $n_c^u$ dla użytkownika $u$ to liczba par przedmiotów, które zostały prawidłowo uporządkowane przez ranker. Innymi słowy, gdy mamy prawdziwy ranking ocen użytkownika oraz przewidywany, to jest to liczba par przedmiotów, które prawidłowo ułożyliśmy (lepszy przedmiot wyżej niż gorszy).\n",
    "$$\\large\n",
    "n_c(u) = |\\{ (i,j) | \\hat{r}_{ui} > \\hat{r}_{uj} \\text{ and } r_{ui} > r_{uj}\\}|\n",
    "$$\n",
    "\n",
    "Pary niezgodne (*discordant pairs*) liczy się podobnie:\n",
    "$$\\large\n",
    "n_d(u) = |\\{ (i,j) | \\hat{r}_{ui} > \\hat{r}_{uj} \\text{ and } r_{ui} \\leq r_{uj}\\}|\n",
    "$$\n",
    "\n",
    "Proporcja par zgodnych do wszystkich, zsumowana dla wszystkich użytkowników, to FCP:\n",
    "$$\\large\n",
    "FCP = \\frac{n_c}{n_c + n_d} = \\frac{\\sum_{i=1}^n n_c(u_i)}{\\sum_{i=1}^n \\left(n_c(u_i) + n_d(u_i)\\right)}\n",
    "$$\n",
    "\n",
    "Można także obliczyć FCP@k, ograniczając się do pierwszych k predykcji.\n",
    "\n",
    "Metryka FCP przykłada mniejszą wagę niż MAP@k do tego, żeby najlepsze przedmioty były jak najwyżej. Skupia się natomiast na tym, żeby lepsze przedmioty były powyżej gorszych. Działa więc lepiej dla rekomendacji dłuższych list, kiedy pierwsze pozycje nie są aż tak ważne, np. przy rekomendowaniu playlist muzyki (Spotify)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNGnlpRo6dn7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Zadanie 4 (1 punkt)**\n",
    "\n",
    "Uzupełnij kod funkcji `ap_k`, która oblicza AP@k dla pojedynczego użytkownika. Pamiętaj, aby ograniczyć się do najwyższych (pierwszych) `k` przedmiotów. W przypadku, gdy model nie miał żadnej dobrej predykcji ($r_k = 0$), zwróć 0.\n",
    "\n",
    "Następnie oblicz i wypisz MAP@k oraz FCP (k=10) dla naszego modelu średniej przedmiotu. Wartości podaj w procentach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xohiz1Rj6dn7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def ap_k(y_true: List[int], y_pred: List[int], k: int) -> float:\n",
    "    number_of_relevant_items_at = lambda cut_off: len([y_p for y_p in y_pred[:cut_off] if y_p in y_true[:cut_off]])  \n",
    "    p_at = lambda cut_off: number_of_relevant_items_at(cut_off) / cut_off\n",
    "    is_pred_relevant = lambda ix: y_pred[ix] in y_true[:k]\n",
    "\n",
    "    r_k = number_of_relevant_items_at(k)\n",
    "    return 0 if r_k == 0 else sum([p_at(i) for i in range(1, k+1) if is_pred_relevant(i-1)]) / r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aeud4ric6dn7",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "from surprise.accuracy import fcp\n",
    "\n",
    "\n",
    "def map_k(df: pd.DataFrame, k: int) -> float:\n",
    "    ap_k_values = []\n",
    "    for idx, row in df.iterrows():\n",
    "        actual, recommendations = row\n",
    "        ap_k_val = ap_k(actual, recommendations, k)\n",
    "        ap_k_values.append(ap_k_val)\n",
    "\n",
    "    return np.mean(ap_k_values)\n",
    "\n",
    "\n",
    "def fcp_k(predictions: List[Prediction], k: int) -> float:\n",
    "    top_k = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_k[uid].append((iid, est))\n",
    "\n",
    "    user_item_id_pairs = set()\n",
    "\n",
    "    for user_id, user_ratings in top_k.items():\n",
    "        user_ratings.sort(key=itemgetter(1), reverse=True)\n",
    "        for item_id, rating in user_ratings[:k]:\n",
    "            user_item_id_pairs.add((user_id, item_id))\n",
    "\n",
    "    predictions_top_k = [\n",
    "        pred for pred in predictions if (pred[0], pred[1]) in user_item_id_pairs\n",
    "    ]\n",
    "\n",
    "    return fcp(predictions_top_k, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kCHwBcw2Wylo"
   },
   "outputs": [],
   "source": [
    "def print_metrics(\n",
    "    predictions: List[Prediction], recommendations: pd.DataFrame, k: int = 10\n",
    ") -> None:\n",
    "    rmse(predictions, verbose=True)\n",
    "    mae(predictions, verbose=True)\n",
    "    map_k_value = map_k(recommendations, k=k)\n",
    "    fcp_k_value = fcp_k(predictions, k=k)\n",
    "\n",
    "    print(f\"MAP@k ({k=}): {100 * map_k_value:.2f}%\")\n",
    "    print(f\"FCP@k ({k=}): {100 * fcp_k_value:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRujyrwD6dn7",
    "outputId": "f9f9155e-6753-41c1-fc2a-4821d68a630c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print_metrics(pred_item_avg, rec_item_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URt5a78s6dn7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Zobaczymy, że ten wynik da się jeszcze poprawić.\n",
    "\n",
    "Metryki MAP@k i MAR@k mają jednak pewną wadę - preferują sugerowanie popularnych treści przez model, bo można je łatwo umieścić wysoko w rekomendacji i łatwo podbić sobie precyzję. W ten sposób rekomendacje byłyby słabo personalizowane. Dlatego wykorzystuje się szereg innych metryk, głównie biorących pod uwagę różnorodność i personalizację rekomendacji, na przykład:\n",
    "- pokrycie (*coverage*) - procent przedmiotów ze zbioru, który nasz system w ogóle rekomenduje,\n",
    "- nowość (*novelty*) - zdolność systemu do rekomendacji zaskakujących, nowych dla użytkownika przedmiotów,\n",
    "- personalizacja (*personalization*) - miara różnicy między rekomendacjami dla poszczególnych użytkowników.\n",
    "\n",
    "Możesz o nich poczytać więcej na stronie [recmetrics](https://github.com/statisticianinstilettos/recmetrics) oraz w [tym artykule](https://towardsdatascience.com/evaluation-metrics-for-recommender-systems-df56c6611093)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrXAThX86dn7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Pomiar jakości systemów rekomendacyjnych - podsumowanie**\n",
    "\n",
    "1. Poza stosowaniem zwykłego podziału losowego train-test można też stosować podział czasowy lub per użytkownik.\n",
    "2. Jednym z najprostszych modeli i dobrym punktem odniesienia (baseline) jest przewidywanie średniej per przedmiot.\n",
    "3. Podstawowymi metrykami jakości są metryki dla regresji: RMSE i MAE, oraz rankowania: MAP@k, MAR@k, NDCG.\n",
    "4. Inne metryki, specyficzne dla rekomendacji, biorą pod uwagę jakość personalizowanych rekomendacji, np. pokrycie, nowość, personalizacja."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXA2VX6q6dn7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model średniej bayesowskiej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYmhwgU26dn7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Masz ochotę na dobrą pizzę i szukasz opinii na Google Maps. Masz do wyboru 2 lokale: jeden ze średnią 5.0 i drugi ze średnią 4.8. Zauważasz jednak, że pierwszy ma tylko 5 opinii, a drugi 200. Który wybierzesz? Są spore szanse, że ten drugi, bo mamy większą **pewność (confidence)** co do oceny takiego lokalu.\n",
    "\n",
    "Prosty model, taki jak średnia przedmiotu, ma ten sam problem, co powyżej. Sformalizowaniem idei \"chcę być pewny, że ocena przedmiotu jest wysoka\" jest model **średniej bayesowskiej (Bayesian average)**. Możliwych sformułowań bayesowskich jest dużo, ale ogólna idea jest zawsze taka, aby wziąć pod uwagę rozkład ocen przedmiotu oraz ich liczbę. Co ważne, to dalej są rekomendacje globalne - mamy jedną predykcję per przedmiot.\n",
    "\n",
    "Czemu średnia \"bayesowska\"? Przypomnijmy sobie twierdzenie Bayesa:\n",
    "$$\\large\n",
    "P(Y|X) = \\frac{P(X|Y) \\cdot P(Y)}{P(X)}\n",
    "$$\n",
    "\n",
    "W naszym wypadku:\n",
    "1. $X$ - zbiór danych, który jest stały.\n",
    "2. $Y$ - przewidywane wartości.\n",
    "3. $P(X)$ - prawdopodobieństwo zaobserwowania naszych danych, które co prawda ciężko jest zmierzyć, ale na szczęście w ML zwykle możemy zignorować mianownik, bo to tylko stała.\n",
    "4. $P(Y)$ - *prior* (*prior distribution*), czyli z góry założony rozkład prawdopodobieństw wartości, które przewidujemy. Często zaczynamy bez żadnej wiedzy, więc zakładamy rozkład jednostajny lub normalny.\n",
    "5. $P(X|Y)$ - *likelihood*, wiarygodność, czyli jak dobrze model odwzorowuje dotychczas zaobserwowane dane.\n",
    "6. $P(Y|X)$ - *posterior* (*posterior distribution*), czyli docelowy rozkład wartości przewidywanych, obliczony na podstawie danych.\n",
    "\n",
    "W kontekście systemów rekomendacyjnych:\n",
    "- $P(Y)$ (prior) to założony z góry rozkład ocen, typowo jednostajny, czyli jest taka sama szansa na każdą ocenę\n",
    "- $P(X|Y)$ (likelihood) to miara, jak dobrze nasz model odwzorowuje macierz ocen; jakbyśmy potraktowali go jako skrzynkę generującą oceny, to wiarygodność mierzy, jak bliskie są te generowane wartości wobec prawdziwych ze zbioru danych\n",
    "- $P(Y|X)$ (posterior) to rozkład przewidywanych ocen dla poszczególnych przedmiotów\n",
    "\n",
    "Jak widać, dostajemy rozkład w wyniku. Jak dostać konkretną predykcję, czyli np. liczbę gwiazdek? Używamy **maximum a posteriori (MAP)**, czyli bierzemy po prostu tę ocenę, dla której rozkład posterior ma największą wartość.\n",
    "\n",
    "Wykorzystamy podejście opisane krok po kroku [w tym artykule](https://fulmicoton.com/posts/bayesian_rating/) oraz [tym tutorialu](https://www.algolia.com/doc/guides/managing-results/must-do/custom-ranking/how-to/bayesian-average/), w którym przewidywana ocena dla $i$-tego przedmiotu (po przekształceniach) to:\n",
    "$$\\large\n",
    "r_i = \\frac{C \\cdot m + S_i}{C + c_i}\n",
    "$$\n",
    "\n",
    "gdzie:\n",
    "- $m$ - *globalna średnia* ocen dla wszystkich przedmiotów,\n",
    "- $C$ - *globalna pewność*, hiperparametr, przyjęta minimalna liczba ocen uwiarygadniająca średnią, może być ustalon np. jako wartość pierwszego kwartyla liczby ocen dla wszystkich produktów w bazie,\n",
    "- $S_i$ - *suma ocen* dla produku $i$,\n",
    "- $c_i$ - *liczba ocen* dla produktu $i$.\n",
    "\n",
    "Dodatkowe źródła:\n",
    "- [artykuł o twierdzeniu Bayesa](https://towardsdatascience.com/understand-bayes-rule-likelihood-prior-and-posterior-34eae0f378c5)\n",
    "- [proste i przyjazne sformułowanie średniej bayesowskiej](https://arpitbhayani.me/blogs/bayesian-average)\n",
    "- [bardziej wyrafinowane podejście oparte o dolną granicę błędu](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html)\n",
    "- [bardzo wyrafinowane podejście oparte o dolną granicę błędu - dla odważnych](https://www.evanmiller.org/ranking-items-with-star-ratings.html)\n",
    "- [średnia bayesowska dla danych zmiennych w czasie](https://www.evanmiller.org/bayesian-average-ratings.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tCn5SBtl6dn7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Zadanie 5 (1 punkt)**\n",
    "\n",
    "Uzupełnij kod klasy `BayesianAveragePredictor`. W metodzie `.fit()` musisz obliczyć parametry:\n",
    "- globalną średnią ocen,\n",
    "- sumę ocen dla każdego przedmiotu,\n",
    "- liczbę ocen dla każdego przedmiotu,\n",
    "- globalną pewność (confidence, $C$).\n",
    "\n",
    "Pewność oblicz jako dolny kwartyl (25 percentyl) rozkładu liczby ocen przedmiotów, zgodnie z [tym tutorialem](https://www.algolia.com/doc/guides/managing-results/must-do/custom-ranking/how-to/bayesian-average/#how-to-calculate-the-bayesian-average). Przyda ci się funkcja `np.quantile()`.\n",
    "\n",
    "Sugerowane jest używanie słowników w `.fit()`, żeby mapować `item_id` na odpowiednią wartość.\n",
    "\n",
    "W metodzie `.estimate()` musisz zastosować obliczone parametry we wzorze podanym powyżej.\n",
    "\n",
    "Dokonaj predykcji i oblicz metryki za pomocą podanej funkcji. Skomentuj wynik w porównaniu do przewidywania średniej przedmiotu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bMoPs0PVSdVp",
    "outputId": "82d94336-959a-49ec-e326-5e513c04c2cb"
   },
   "outputs": [],
   "source": [
    "s = pd.Series([1,3,0,7,5],index=[0,1,2,3,4])\n",
    "list(s).index(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUgtCmJ6P-oa",
    "outputId": "19cf1918-7098-44c7-d02c-9eeac8824827"
   },
   "outputs": [],
   "source": [
    "df[[\"item_id\", \"rating\"]].groupby(\"item_id\").size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "53NAowLPEp3c",
    "outputId": "74a1cc88-18cc-4ce3-bd88-52549ed363fb"
   },
   "outputs": [],
   "source": [
    "df[[\"item_id\", \"rating\"]].groupby(\"item_id\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4pBjZWk6dn8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BayesianAveragePredictor(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        self.global_avg_ = trainset.global_mean\n",
    "\n",
    "        self.item_sum_of_ratings = df[[\"item_id\", \"rating\"]].groupby(\"item_id\").sum()\n",
    "        self.item_number_of_ratings = df[[\"item_id\", \"rating\"]].groupby(\"item_id\").size()\n",
    "        self.C = self.item_number_of_ratings.sort_values().quantile(0.25)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible(\"User and/or item is unknown.\")     \n",
    "\n",
    "        C = self.C\n",
    "        m = self.global_avg_\n",
    "        item_sum = self.item_sum_of_ratings[\"rating\"].iloc[i-1]\n",
    "        item_count = self.item_number_of_ratings.iloc[i-1]\n",
    "        score = (C*m + item_sum) / (C + item_count)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vYstYnJ76dn8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def print_metrics(\n",
    "    predictions: List[Prediction], recommendations: pd.DataFrame, k: int = 10\n",
    ") -> None:\n",
    "    rmse(predictions, verbose=True)\n",
    "    mae(predictions, verbose=True)\n",
    "    map_k_value = map_k(recommendations, k=k)\n",
    "    fcp_k_value = fcp_k(predictions, k=k)\n",
    "\n",
    "    print(f\"MAP@k ({k=}): {100 * map_k_value:.2f}%\")\n",
    "    print(f\"FCP@k ({k=}): {100 * fcp_k_value:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTte2p596dn8",
    "outputId": "b271322f-0a17-4ef5-ccfb-16950aa598c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "algo = BayesianAveragePredictor()\n",
    "algo.fit(train_set)\n",
    "pred_bayes_avg = algo.test(test_set)\n",
    "rec_bayes_avg = get_recommendations(pred_bayes_avg)\n",
    "\n",
    "# calculate and print metrics\n",
    "\n",
    "print_metrics(pred_bayes_avg, rec_bayes_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gAPrRWwi6dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "RMSE wzrost z 1 do 1.2\n",
    "\n",
    "MAE: wzrost z 0.8 do 0.9\n",
    "\n",
    "MAP@k: bardzo podobny\n",
    "\n",
    "FCP@k: spadek o 10% \n",
    "\n",
    "Otrzymane wyniki w tym przypadku są trochę gorsze od modelu średniej."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWQu0DBu6dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metody oparte o sąsiedztwo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ff4o90Q96dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Mając solidne punkty odniesienia w postaci rekomendacji globalnych, możemy przejść do rekomendacji personalizowanych. W metodach **opartych o sąsiedztwo (neighborhood-based)** znajdujemy podobnych użytkowników do nas, albo przedmioty podobne do tych które lubiliśmy, i na podstawie tego dokonujemy rekomendacji.\n",
    "\n",
    "Podejście to jest używane także w innych obszarach uczenia maszynowego, np. w algorytmie k najbliższych sąsiadów (*k nearest neighbors*, kNN), SMOTE, albo w identyfikacji osób (znajdujemy 1 najbliższego sąsiada dla osadzenia twarzy). Wymaga ono odpowiedniej metryki, która zmierzy odległość między wektorami, znajdując k najbliższych sąsiadów, z których następnie wyciągamy informacje."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPVARhvW6dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### User-based neighborhood-based CF\n",
    "\n",
    "Idea podejścia **user-based** jest bardzo prosta - znajdźmy użytkowników podobnych do nas, którzy oceniali przedmioty, których my jeszcze nie widzieliśmy i zasugerujmy to, co potencjalnie najbardziej będzie się nam podobać. Realizuje podejście \"użytkownicy podobni do ciebie oglądali także...\".\n",
    "\n",
    "Algorytm user-based collaborative filtering działa następująco:\n",
    "1. Dla każdego użytkownika znajdź k najbliższych sąsiadów.\n",
    "2. Predykcja dla przedmiotu to średnia ocena sąsiadów dla tego przedmiotu, którzy ocenili dany przedmiot.\n",
    "3. Zarekomenduj te przedmioty, które mają najwyższą przewidywaną ocenę.\n",
    "\n",
    "Co ważne, przy obliczaniu najbliższych użytkowników bierzemy tylko te przedmioty, które obaj ocenili. Przykładowo, jeżeli użytkownik $u_1$ ocenił przedmioty $[1, 2, 3]$, a użytkownik $u_2$ ocenił przedmioty $[2, 3, 4]$, to na potrzeby obliczania ich podobieństwa bierzemy pod uwagę tylko $[2, 3]$. Przy obliczaniu predykcji dla $i$-tego przedmiotu także bierzemy pod uwagę tylko tych najbliższych sąsiadów, którzy wystawili mu ocenę.\n",
    "\n",
    "Predykcja dla użytkownika $u$ i przedmiotu $i$ to:\n",
    "$$\\large\n",
    "\\hat{r}_{ui} = \\frac{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v) * r_{vi}}{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v)}\n",
    "$$\n",
    "gdzie:\n",
    "- $N_i^k(u)$ - $k$ najbliższych sąsiadów dla użytkownika $u$, którzy ocenili przedmiot $i$\n",
    "- $r_{vi}$ - ocena przedmiotu $i$ przez użytkownika $v$\n",
    "- $\\text{sim}(u, v)$ - podobieństwo użytkowników $u$ i $v$ według metryki $\\text{sim}$\n",
    "\n",
    "Co ważne, tutaj metryka jest podobieństwem, tzn. większa wartość = bardziej podobni użytkownicy. Typowo używa się **korelacji Pearsona (Pearson correlation)**, która przyjmuje wartości z zakresu $[-1, 1]$. Dzięki temu wiemy, którzy użytkownicy są bardzo podobni (blisko 1), którzy mają wręcz przeciwny gust do naszego (blisko -1), a którzy są w ogóle inni od nas (blisko 0). Niektóre implementacje (np. Surprise) biorą pod uwagę tylko sąsiadów o nieujemnej korelacji, a inne wykorzystują tę informację z ujemną wagą.\n",
    "\n",
    "Poniższy algorytm wykorzystuje podstawową implementację k najbliższych sąsiadów z biblioteki Surprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bpgFLec96dn8",
    "outputId": "240914b1-c4f6-4533-9ce5-d6e92ed3f0fe",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.knns import KNNBasic\n",
    "\n",
    "knn_basic = KNNBasic()\n",
    "knn_basic.fit(train_set)\n",
    "pred_knn_basic = knn_basic.test(test_set)\n",
    "rec_knn_basic = get_recommendations(pred_knn_basic)\n",
    "\n",
    "print_metrics(pred_knn_basic, rec_knn_basic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVHvgJmF6dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wynik nie jest zbyt dobry, ale nie przeprowadziliśmy jeszcze żadnego tuningu hiperparametrów.\n",
    "\n",
    "Najważniejszym hiperparametrem jest **liczba sąsiadów `k`**. Trzeba wziąć pod uwagę, że nie wszystkie przedmioty będą się pokrywać między użytkownikami, więc typowo bierze się ich dość dużo. Jeżeli dana implementacja uwzględnia tylko nieujemne korelacje, to jeszcze więcej sąsiadów może odpaść, więc trzeba wziąć większą wartość. Jest to więc de facto maksymalna liczba sąsiadów do uwzględnienia. Im większa wartość, tym mocniejsza regularyzacja, bo uśredniamy więcej użytkowników. Przede wszystkim należy jednak wziąć pod uwagę wielkość naszego zbioru, szczególnie liczbę użytkowników oraz gęstość.\n",
    "\n",
    "Drugim hiperparametrem jest **minimalna liczba sąsiadów `min_k`**. Jeżeli spośród `k` najbliższych sąsiadów mniej niż `min_k` oceniło dany przedmiot, to mamy zimny start. Zwykle wykorzystuje się wtedy algorytm globalny, np. przewidując globalną średnią. Jak widać, system rekomendacyjny składa się w środku z bardzo wielu systemów rekomendacyjnych :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRSQA7Rs6dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Zadanie 6 (1 punkt)**\n",
    "\n",
    "Przeprowadź tuning hiperparametrów, używając 10-krotnej walidacji skrośnej i optymalizując MAE. Jako że nasz zbiór jest dość mały, to sprawdzimy zakres:\n",
    "```\n",
    "param_grid = {\n",
    "    \"k\": list(range(10, 51, 10)),\n",
    "    \"min_k\": list(range(1, 4)),\n",
    "    \"random_state\": [0],\n",
    "    \"verbose\": [False]\n",
    "}\n",
    "```\n",
    "\n",
    "Jako że interesują nas przede wszystkim same rekomendacje, optymalizuj metrykę FCP. Wypisz znalezione najlepsze hiperparametry oraz metryki na zbiorze testowym dla najlepszego modelu.\n",
    "\n",
    "Wskazówki:\n",
    "- użyj `GridSearchCV` z biblioteki Surprise,\n",
    "- argument `refit` ma domyślną wartość `False`, inaczej niż w Scikit-learn'ie,\n",
    "- argument `n_jobs`\n",
    "- `random_state` trzeba przekazać jako hiperparametr, API Surprise jest tutaj niezbyt dobrze zrobione ([Github issue](https://github.com/NicolasHug/Surprise/issues/212)),\n",
    "- analogicznie do powyższego działa przekazywanie `verbose` (żeby uniknąć zalewu tekstu).\n",
    "\n",
    "Skomentuj wyniki i zmiany w poszczególnych metrykach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UGayeYWq6dn8",
    "outputId": "1a5f78a2-ccdd-46c5-ba08-50663d232bcf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"k\": list(range(10, 51, 10)),\n",
    "    \"min_k\": list(range(1, 4)),\n",
    "    \"random_state\": [0],\n",
    "    \"verbose\": [False]\n",
    "}\n",
    "gs = GridSearchCV(KNNBasic, param_grid, measures=[\"fcp\"], refit=True, n_jobs=-1)\n",
    "\n",
    "gs.fit(data_train)\n",
    "\n",
    "print(gs.best_score)\n",
    "print(gs.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MgLtiG1dFcI",
    "outputId": "c684587a-456e-4800-87fd-521fa6fff55d"
   },
   "outputs": [],
   "source": [
    "algo = gs.best_estimator['fcp']\n",
    "pred = algo.test(test_set)\n",
    "rec = get_recommendations(pred)\n",
    "print_metrics(pred, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wIcqwqA6dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Udało się nieznacznie poprawić metryki. Bardzo niska czułość na hiperparametry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l6J2hkfc6dn8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ten algorytm nie bierze jednak psychologicznych różnic między użytkownikami. Niektórzy użytkownicy będą średnio zawyżać oceny, bo film to dla nich luźna rozrywka, a poważni koneserzy mogą dawać filmom średnio dość niskie oceny. Taka tendencja to **user bias**, ale na szczęście można go policzyć - to po prostu średnia ocena wystawiana przez użytkownika, a więc średnia z każdego wiersza w macierzy ocen.\n",
    "\n",
    "Jeżeli od każdego wiersza odejmiemy jego średnią, to dostaniemy **ratings deviations**, czyli nie mamy już w macierzy samych ocen, tylko jak bardzo ocena danego przedmiotu przez użytkownika różni się od jego średniej predykcji. Taka operacja to **centrowanie (centering)**. Na takich wartościach można też zwyczajnie liczyć najbliższych sąsiadów, a korelacja Pearsona dalej działa dla takich danych. Żeby dokonać predykcji, przewidujemy odchylenie dla przedmiotu, a następnie dodajemy je dla średniej danego użytkownika.\n",
    "\n",
    "Mamy zatem:\n",
    "$$\\large\n",
    "\\hat{r}_{ui} = \\mu_i + \\frac{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v) * (r_{vi} - \\mu_v)}{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v)}\n",
    "$$\n",
    "\n",
    "Alternatywnie możemy zastosować **standaryzację (standardization)**, czyli odejmujemy średnią i dzielimy przez odchylenie standardowe dla użytkownika. Inną nazwą na taką transformację jest Z-score. Daje to wzór:\n",
    "$$\\large\n",
    "\\hat{r}_{ui} = \\mu_i + \\sigma_i \\cdot \\frac{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v) * \\frac{r_{vi} - \\mu_v}{\\sigma_v}}{\\sum_{v \\in N_i^k(u)} \\text{sim}(u, v)}\n",
    "$$\n",
    "\n",
    "Można traktować rodzaj normalizacji jako hiperparametr, ale zazwyczaj samo odjęcie średniej wystarcza i daje lepsze wyniki od zwykłego kNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M4kavSOv6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Zadanie 7 (1 punkt)**\n",
    "\n",
    "Analogicznie do poprzedniego zadania wytrenuj, zoptymalizuj i sprawdź na zbiorze treningowym user-based CF z centrowaniem (`KNNWithMeans`) oraz ze standaryzacją (`KNNWithZScore`). Wypisz także optymalny zestaw hiperparametrów dla obu algorytmów. Wykorzystaj tę samą siatkę hiperparametrów, co w poprzednim zadaniu.\n",
    "\n",
    "Skomentuj uzyskane hiperparametry i wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKA2pu1udy7l"
   },
   "outputs": [],
   "source": [
    "def test_and_print_metrics(model):\n",
    "    pred = model.test(test_set)\n",
    "    rec = get_recommendations(pred)\n",
    "    print_metrics(pred, rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I3pqXIk06dn9",
    "outputId": "8ab2fdb4-e910-481a-c592-b6fa9c7adb1b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.knns import KNNWithMeans, KNNWithZScore\n",
    "gs = GridSearchCV(KNNWithMeans, param_grid, measures=[\"fcp\"], refit=True, n_jobs=-1)\n",
    "gs.fit(data_train)\n",
    "print(gs.best_score)\n",
    "print(gs.best_params)\n",
    "test_and_print_metrics(gs.best_estimator[\"fcp\"])\n",
    "\n",
    "print(\"========\")\n",
    "\n",
    "gs = GridSearchCV(KNNWithZScore, param_grid, measures=[\"fcp\"], refit=True, n_jobs=-1)\n",
    "gs.fit(data_train)\n",
    "print(gs.best_score)\n",
    "print(gs.best_params)\n",
    "test_and_print_metrics(gs.best_estimator[\"fcp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nM58CXXN6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Uzyskane wyniki są zbliżone do tych uzyskanych wcześniej, co sugeruje, że model nie jest bardzo czuły na hiperparametry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRR7FcHU6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Item-based neighborhood-based CF\n",
    "\n",
    "Idea podejścia **item-based** jest bardzo podobna do user-based, ale znajdujemy podobne przedmioty, a nie użytkowników. Operujemy zatem na kolumnach macierzy ocen. Realizuje to podejście \"mogą cię jeszcze zainteresować przedmioty...\" oraz \"skoro oglądałeś X, to mogą spodobać ci się...\".\n",
    "\n",
    "Predykcja dla użytkownika $u$ i przedmiotu $i$ to:\n",
    "$$\\large\n",
    "\\hat{r}_{ui} = \\frac{\\sum_{j \\in N_u^k(i)} \\text{sim}(u, v) * r_{uj}}{\\sum_{j \\in N_u^k(i)} \\text{sim}(u, v)}\n",
    "$$\n",
    "\n",
    "Podobieństwo przedmiotów liczymy tutaj według kolumn macierzy, a metryką jest zwykle **podobieństwo cosinusowe (cosine similarity)**. Wykorzystuje się także centrowanie, eliminując **item bias** - przykładowo, \"Titanic\" będzie miał zwykle zawyżone oceny, bo każdy słyszał, że to znany i dobry film, więc podświadomie zawyżymy mu ocenę. Metrykę po centralizacji nazywa się czasem *adjusted cosine similarity*.\n",
    "\n",
    "Podejście item-based zazwyczaj daje większą dokładność niż user-based, tzn. niższe RMSE i MAE. Skutkuje to jednak niższym pokryciem czy nowością. Takie podejście potrafi być też bardziej czułe na zimny start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ovUU1oz6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Zadanie 8 (1 punkt)**\n",
    "\n",
    "Zaimplementuj podejście item-based z metryką cosinusową w wariantach:\n",
    "- bez normalizacji,\n",
    "- z centrowaniem (adjusted cosine),\n",
    "- ze standaryzacją.\n",
    "\n",
    "Analogicznie do poprzedniego ćwiczenia zastosuj optymalizację hiperparametrów, podaj najlepszy zestaw oraz wypisz metryki na zbiorze testowym. Wykorzystaj tę samą siatkę hiperparametrów, co w podobnym ćwiczeniu, zmieniając tylko odpowiednio opcje metryki `sim_options`.\n",
    "\n",
    "Żeby zamienić algorytm user-based na item-based, oraz zmienić metrykę, przyda ci się [ten tutorial](https://surprise.readthedocs.io/en/stable/getting_started.html#tune-algorithm-parameters-with-gridsearchcv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wd1aV0o6dn9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"k\": list(range(10, 51, 10)),\n",
    "    \"min_k\": list(range(1, 4)),\n",
    "    \"random_state\": [0],\n",
    "    \"verbose\": [False],\n",
    "    \"sim_options\": {\n",
    "        \"name\": [\"cosine\"],\n",
    "        \"user_based\": [False]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "esrk5-d8gWE2",
    "outputId": "0d650fdb-392d-47c0-957a-41f21a23e577"
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(KNNBasic, param_grid, measures=[\"fcp\"], refit=True, n_jobs=-1)\n",
    "gs.fit(data_train)\n",
    "print(gs.best_score)\n",
    "print(gs.best_params)\n",
    "test_and_print_metrics(gs.best_estimator[\"fcp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ohIGafRggU3",
    "outputId": "7aae1075-5a7c-438e-9d43-b91240d07629"
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(KNNWithMeans, param_grid, measures=[\"fcp\"], refit=True, n_jobs=-1)\n",
    "gs.fit(data_train)\n",
    "print(gs.best_score)\n",
    "print(gs.best_params)\n",
    "test_and_print_metrics(gs.best_estimator[\"fcp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MUr4LLNjgiig",
    "outputId": "c78ff775-2d02-4258-db99-b3791c5d31b3"
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(KNNWithZScore, param_grid, measures=[\"fcp\"], refit=True, n_jobs=-1)\n",
    "gs.fit(data_train)\n",
    "print(gs.best_score)\n",
    "print(gs.best_params)\n",
    "test_and_print_metrics(gs.best_estimator[\"fcp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruxBEJAv6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Metody oparte o sąsiedztwo - podsumowanie\n",
    "\n",
    "Podsumowanie:\n",
    "1. Możemy wyróżnić dwa sposoby liczenia sąsiadów: user-based (inni użytkownicy, wiersze macierzy) oraz item-based (inne przedmioty, kolumny macierzy).\n",
    "2. Podejście user-based wykorzystuje zwykle korelację Pearsona, a item-based podobieństw cosinusowe.\n",
    "3. Użytkownicy oraz przedmioty mają naturalne obciążenie (user bias, item bias), które można wyeliminować, stosując normalizację: centrowanie lub standaryzację.\n",
    "\n",
    "Zalety:\n",
    "1. Prostota.\n",
    "2. Interpretowalność, szczególnie dla item-based.\n",
    "3. Stosunkowo niewielka czułość na dobór hiperparametrów.\n",
    "4. Możliwość idealnego uwspółbieżnienia treningu oraz predykcji (embarassingly parallel).\n",
    "\n",
    "Wady:\n",
    "1. Dość trudna implementacja, trzeba wybierać wspólne przedmioty.\n",
    "2. Trening jest niezbyt skalowalny dla bardzo dużych danych.\n",
    "3. Czułe na zimny start.\n",
    "4. Bardzo niewiele implementacji wspiera inkrementacyjne dodawanie nowych użytkowników/przedmiotów - trzeba przetrenowywać regularnie cały model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmTV16E06dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Metody oparte o rozkład macierzy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xm9odfOK6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Podejście najbliższych sąsiadów definiuje \"sąsiedztwo\" bardzo explicite - wymaga, by użytkownicy ocenili dokładnie te same filmy, aby w ogóle sprawdzać, czy są podobni. Nie wykorzystuje to niejawnych podobieństw między przedmiotami i filmami. Przykładowo, jeżeli jeden lubi filmy \"Szeregowiec Ryan\", \"Dunkierka\" i \"Wróg u bram\", a drugi lubi filmy \"Czas apokalipsy\" i \"Jak rozpętałem drugą wojnę światową\", to są do siebie bardzo podobni, a jednak podejście user-based nawet nie będzie w stanie tego sprawdzić. Item-based mogłoby tu nieco pomóc, ale tam mogą się zdarzyć analogiczne sytuacje.\n",
    "\n",
    "Podejście oparte o rozkład macierzy, spopularyzowane w ramach konkursu Netflix Prize 2007 przez Simona Funka ([wywiad](https://www.kdd.org/exploration_files/simon-funk-explorations.pdf), [jego blog](https://sifter.org/simon/journal/20061211.html)), rozwiązuje właśnie ten problem. Stanowi kamień milowy w systemach rekomendacyjnych, gdyż daje bardzo dobre wyniki, doskonale uwspółbieżnia się i rozprasza na wiele maszyn, a do tego jest naprawdę proste. Szczegółowy i bardzo przystępny opis tego podejścia można znaleźć w artykule [\"Matrix factorization techniques for recommender systems\" Y. Koren, R. Bell, C. Volinsky](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf).\n",
    "\n",
    "Ideą jest, aby dokonać **rozkładu macierzy (matrix decomposition)** na macierzy ocen $R$, przybliżając ją jako iloczyn dwóch macierzy $W$ (user matrix) i $U$ (item matrix), które reprezentują użytkowników i przedmioty po dekompozycji:\n",
    "$$\\large\n",
    "\\hat{R} = WU^T\n",
    "$$\n",
    "\n",
    "\n",
    "Załóżmy, że mamy $N$ użytkowników i $M$ przedmiotów. Kształty macierzy to zatem:\n",
    "$$\\large\n",
    "\\hat{R}_{N \\times M} = W_{N \\times K} U_{K \\times N}^T\n",
    "$$\n",
    "\n",
    "Pojawił nam się nowy wymiar $K$ - każdy użytkownik to teraz wektor z macierzy $W$ o długości $K$, a każdy przedmiot to wektor z macierzy $U$ o długości $K$. Jest to **ukryta wymiarowość (latent dimensionality)**, stanowiąca hiperparametr, analogiczny np. do rozmiaru warstw sieci neuronowej. Wartości wektórów nie są interpretowalne, ale dla dobrych modeli można zauważyć, że odwzorowują pewne ogólne tematy w danych. Przykładowo, dla filmów mogą oznaczać, jak gatunki filmowe, np. \"romans\", \"komedia\", \"akcja\". Dla użytkowników mogą oznaczać, w jakim stopniu użytkownik interesuje się danym gatunkiem. Typowe wartości $K$ leżą w przedziale od kilkadziesięci do kilkuset. Ze względu na wykorzystanie wymiarów niejawnych takie modele nazywa się też **latent factor models**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYBb0RJp6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "Predykcji w takim modelu dokonuje się przez iloczyn skalarny wektora użytkownika (wiersz $W$) z wektorem przedmiotu (kolumna $U$):\n",
    "$$\\large\n",
    "r_{ui} = w_u^Tu_i\n",
    "$$\n",
    "\n",
    "Dzięki takiemu sformułowaniu, jeśli zainteresowanie użytkownika tematem $k$ będzie duże, a film będzie dobrze wpasowywał się w temat $k$, to ich pomnożenie da dużą wartość, a zatem wysoką wartość rekomendacji.\n",
    "\n",
    "Algorytm ten nazywa się czasem niepoprawnie SVD, bo takiej nazwy użył Simon Funk do opisu swojego algorytmu (jego wersja ma trochę ulepszeń; będziemy ją nazywać FunkSVD). Co ważne, nie wykorzystujemy tutaj algorytmu SVD, bo nie potrzebujemy całego jego aparatu matematycznego. Zamiast tego ten algorytm to po prostu **matrix factorization (MF)**, tudzież **Probabilistic Matrix Factorization (PMF)** ([oryginalny artykuł PMF](https://proceedings.neurips.cc/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf) dowodzi, że to sformułowanie jest poprawne probabilistycznie). Trenuje się go także bez SVD, zamiast tego wykorzystując spadek wzdłuż gradientu lub algorytm **Alternating Least Squares (ALS)**. Ciężko powiedzieć, które podejście jest lepsze, patrz np. [ta dyskusja](https://stats.stackexchange.com/questions/201279/comparison-of-sgd-and-als-in-collaborative-filtering), [ten artykuł](http://cs229.stanford.edu/proj2014/Christopher%20Aberger,%20Recommender.pdf). \n",
    "Oba podejścia bardzo dobrze opisuje [ten artykuł](https://datasciencemadesimpler.wordpress.com/tag/alternating-least-squares/), który rozwija też bardziej formalnie, czemu ekstrakcja \"tematów\" działa (spoiler: MF dokonuje implicite klasteryzacji).\n",
    "\n",
    "Niezależnie od podejścia, celem algorytmu jest minimalizacja funkcji kosztu, czyli różnicy między naszym przybliżeniem $\\hat{R} = WU^T$ a prawdziwą macierzą $R$. Zwykle wykorzystuje się tutaj błąd średniokwadratowy, w zapisie macierzowym $||\\hat{R} - R||_2^2$. \n",
    "\n",
    "Formalnie:\n",
    "$$\\large\n",
    "L = \\sum_{u, i \\in \\Omega} \\left( r_{ui} - \\hat{r}_{ui} \\right)^2\n",
    "$$\n",
    "gdzie $\\Omega$ to zbiór wszystkich wypełnionych komórek w macierzy ocen.\n",
    "\n",
    "Jako że mamy dwie macierze do nauczenia, $W$ oraz $U$, to mamy pochodną po wektorach $w$ oraz po wektorach $u$, czyli wierszach macierzy W i U.\n",
    "\n",
    "Po przekształceniach dostajemy (gdzie $w_i$ to $i$-ty wiersz macierzy $W$, a $u_j$ to $j$-ty wiersz macierzy $U$):\n",
    "$$\\large\n",
    "w_i = \\left( \\sum_{j \\in \\Psi_i} u_ju_j^T \\right)^{-1} \\sum_{j \\in \\Psi_i} r_{ij}u_j\n",
    "$$\n",
    "\n",
    "&nbsp; \n",
    "\n",
    "$$\\large\n",
    "u_j = \\left( \\sum_{i \\in \\Omega_j} w_iw_i^T \\right)^{-1} \\sum_{i \\in \\Omega_j} r_{ij}w_i\n",
    "$$\n",
    "gdzie:\n",
    "- $\\Psi_i$ oznacza zbiór przedmiotów, które ocenił użytkownik $i$,\n",
    "- $\\Omega_j$ oznacza zbiór użytkowników, którzy ocenili przedmiot $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36pjpolO6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Można zauważyć, że w obu przypadkach jest to zwyczajny nadokreślony (*overdetermined*) układ równań postaci $x=A^{-1}b$. Taki problem rozwiązuje się metodą najmniejszych kwadratów (*ordinary least squares*), stąd część nazwy metody. Oczywiście do rozwiązania problemu OLS można użyć SVD.\n",
    "\n",
    "Przybliżymy teraz krótko metodę ALS, bo SGD znamy już z sieci neuronowych. Można zauważyć w powyższych wzorach, że dla $W$ pochodna zależy od wartości w $U$, a dla $U$ od wartości w $W$ - wydaje się, że sytuacja patowa. Rozwiązaniem jest po prostu przyjąć losowy punkt wyjścia, a potem raz rozwiązywać $W$ za pomocą $U$, a raz na odwrót.\n",
    "\n",
    "Pełny algorytm ALS:\n",
    "1. Zainicjalizuj losowo macierze $W$ i $U$ niewielkimi wartościami z rozkładu normalnego\n",
    "2. Powtarzaj przez T kroków:\n",
    "  1. Zaktualizuj $U$ według wzoru, rozwiązując układ równań; $W$ jest stałe\n",
    "  2. Zaktualizuj $W$ według wzoru, rozwiązując układ równań; $U$ jest stałe\n",
    "\n",
    "Co ważne, zbieżność i ALS, i spadku wzdłuż gradientu jest gwarantowana, ale do minimum lokalnego. Zwykle nie stanowi to jednak problemu, a w razie czego zawsze można wytrenować wiele modeli na różnych `random_state` i wybrać najlepszy. Liczba epok treningowych stanowi dość prosty hiperparametr - im więcej, tym dokładniejsi po prostu będziemy, więc możemy bardziej overfitować (analogicznie do sieci neuronowych). Zazwyczaj w przypadku ALS wystarcza niewielka liczba, kilka-kilkanaście iteracji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W64XMUc-6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Surprise implementuje wersję z SGD. Wersję z ALS implementuje np. Apache Spark. Wersję z SGD można też łatwo zaimplementować w dowolnym frameworku do sieci neuronowych, np. PyTorch czy TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2p4SODAP6dn9",
    "outputId": "1c66cc50-b31d-4e0d-90b3-e7d91c74074b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.matrix_factorization import SVD\n",
    "\n",
    "# regular MF - no user/item bias, no regularization\n",
    "mf = SVD(biased=False, reg_all=0, random_state=0)\n",
    "mf.fit(train_set)\n",
    "pred_mf = mf.test(test_set)\n",
    "rec_mf = get_recommendations(pred_mf)\n",
    "\n",
    "print_metrics(pred_mf, rec_mf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wDLjbjNr6dn9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Dostaliśmy całkiem dobry wynik bez żadnego tuningu, a czeka nas jeszcze trochę ulepszeń, bo powyższy algorytm to jeszcze nie słynny FunkSVD.\n",
    "\n",
    "Skoro user bias i item bias pomagały w metodach opartych o sąsiedztwo, to dodajmy je też tutaj. Najpierw możemy odjąć od wszystkiego globalną średnią $\\mu$, żeby wycentrować całą macierz. Potem odejmujemy od każdego wiersza user bias $b_u$, a na koniec od każdej kolumny item bias $b_i$. Predykcja to zatem:\n",
    "$$\\large\n",
    "\\hat{r}_{ui} = \\mu + b_u + b_i + w_u^Tu_i\n",
    "$$\n",
    "\n",
    "Drugie ulepszenie to dodanie regularyzacji do naszej funkcji kosztu. W końcu nie możemy się zbyt bardzo dostosować do zbioru treningowego, nasz algorytm ma generalizować się dla przyszłych rekomendacji. Co ważne, mamy tutaj aż 4 możliwe źródła przeuczenia:\n",
    "- $b_u$ - zbytnie dostosowanie do dotychczasowych odchyleń użytkowników,\n",
    "- $b_i$ - analogicznie, ale dla przedmiotów,\n",
    "- $w_i$ - jest to wektor wag, więc duże wagi oznaczają overfitting, jak np. w regresji liniowej,\n",
    "- $u_i$ - analogicznie, ale dla drugiej macierzy.\n",
    "\n",
    "Można by użyć 4 osobnych współczynników regularyzacji, ale optymalizacja takiej siatki hiperparametrów jest raczej mało wykonalna. Można więc użyć jednego hiperparametru na moc regularyzacji L2 $\\lambda$, włączając do niego wszystkie parametry. \n",
    "\n",
    "Daje to funkcję kosztu:\n",
    "$$\\large\n",
    "L = \\sum_{u, i \\in \\Omega} \\left( r_{ui} - \\hat{r}_{ui} \\right)^2 + \\lambda \\left( ||W||_2^2 + ||U||_2^2 + ||b_u||_2^2 + ||b_i||_2^2 \\right)\n",
    "$$\n",
    "\n",
    "Pomijając dalsze wyprowadzenie, nic nie zmienia to w gruncie rzeczy w algorytmie ALS, dalej możemy użyć zwykłego OLS, zmienią się tylko trochę wartości w macierzach. Niewiele zmienia się też, gdy używamy spadku wzdłuż gradientu - dodajemy tylko regularyzację do funkcji kosztu.\n",
    "\n",
    "Powyższe sformułowanie to już pełny algorytm FunkSVD. Zobaczmy, jak sobie poradzi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CGJKGG1v6dn-",
    "outputId": "b35f0d4c-4425-49db-fb87-7390f40f6685",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "funk_svd = SVD(biased=True, random_state=0)\n",
    "funk_svd.fit(train_set)\n",
    "pred_funk_svd = funk_svd.test(test_set)\n",
    "rec_funk_svd = get_recommendations(pred_funk_svd)\n",
    "\n",
    "print_metrics(pred_funk_svd, rec_funk_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8efjJ-G6dn-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Wygląda to na bardzo dobry wynik, a nie dokonaliśmy jeszcze żadnego tuningu hiperparametrów.\n",
    "\n",
    "**Zadanie 9 (1 punkt)**\n",
    "\n",
    "Zaimplementuj tuning hiperparametrów dla algorytmu FunkSVD, sprawdzając siatkę hiperparametrów:\n",
    "```\n",
    "param_grid = {\n",
    "    \"n_factors\": list(range(50, 151, 10)),\n",
    "    \"lr_all\": [0.001, 0.003, 0.005, 0.007, 0.01],\n",
    "    \"reg_all\": [0.01, 0.02, 0.03]\n",
    "}\n",
    "```\n",
    "\n",
    "Pamiętaj, aby przekazać stałe `random_state`! Przyda się też `n_jobs`. Jeżeli na twoim sprzęcie będzie się to liczyć o wiele za długo, to możesz zmniejszyć zakres `n_factors` do 80-121.\n",
    "\n",
    "Skomentuj wyniki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "szC1-rmR6dn-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_factors\": list(range(50, 151, 10)),\n",
    "    \"lr_all\": [0.001, 0.003, 0.005, 0.007, 0.01],\n",
    "    \"reg_all\": [0.01, 0.02, 0.03],\n",
    "    \"random_state\": [0],\n",
    "    \"verbose\": [False]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QmHY9ormj7MZ",
    "outputId": "a3babc0c-2377-4e65-e579-40b992d9ee06"
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(SVD, param_grid, measures=[\"fcp\"], refit=True, n_jobs=-1)\n",
    "gs.fit(data_train)\n",
    "print(gs.best_score)\n",
    "print(gs.best_params)\n",
    "test_and_print_metrics(gs.best_estimator[\"fcp\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9YzWMix6dn-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "// skomentuj tutaj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vssbYyI_6dn-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Pytania kontrolne (2 punkty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4tBoLce6dn-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Każde pytanie jest warte 0.5 punktu.\n",
    "\n",
    "1. Załóżmy, że mamy system rekomendujący reklamy użytkownikom. Reklamy zmieniają się regularnie i dość często. Czy algorytmy collaborative filtering będą tu dobrym wyborem, a jeśli tak, to jakie? Jeżeli nie, to co będzie stanowić tu główny problem?\n",
    "\n",
    "\n",
    "2. Wymyśl i krótko opisz architekturę przykładowego systemu rekomendacyjnego, sugerującego gry w sklepie Steam. Opisz, jakich algorytmów użyłbyś w konkretnych przypadkach i czemu. Uwzględnij, że może wystąpić zimny start w przypadku nowych użytkowników oraz gier, oraz że mamy ogromną przestrzeń możliwych gier - każdy użytkownik zagra tylko w niewielki ułamek.\n",
    "\n",
    "\n",
    "3. Pracujesz w firmie obsługującej platformy z newsami jako data scientist. Zespół data engineering zgromadził bardzo dużą ilość logów o ludziach klikających w artykuły i linki (clickstream data). Użytkownicy nie muszą logować się do systemu, więc identyfikacja jest oparta o ciasteczka (cookies) i niepewna na dłuższą metę. Posiadasz za to bogate metadane o użytkowniku (np. rodzaj urządzenia, przeglądarki, geolokacja, historia sesji) oraz o artykułach (np. język, treść, tagi). Jakiego rodzaju systemu rekomendacyjnego byś użył (jakiego da się użyć?) i dlaczego? Nie musisz tutaj opisywać szczegółowo algorytmów. Rozważ:\n",
    "  - globalny vs personalizowany\n",
    "  - content-based vs collaborative filtering vs hybrydowy\n",
    "  - explicit vs implicit\n",
    "\n",
    "\n",
    "4. Pracujesz w firmie tworzącej oprogramowanie dla sklepów internetowych. Użytkownik ma możliwość sortowania malejąco po średniej opinii, ale z historii użyć wynika, że często nie klikają przedmiotów o najwyższej średniej ocenie, tylko któryś z kolejnych. Jaki może być tego przyczyna? Jak można by rozwiązać ten problem?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RL8ZKGu4mTrb"
   },
   "source": [
    "Odpowiedzi:\n",
    "\n",
    "1. Myślę, że warto sprawdzić. Mogłyby się sprawdzić, biorąc pod uwagę wcześniejszą interakcję użytkowników z reklamami stosująć podejśćie user-based. Głównym problemem z jakim się taki system będzie borykał to cold-start dla nowych reklam i nowych użytkowników. Można to rozwiązać korzystając z content-based podejścia lub spróbować podejścia hybrydowego.\n",
    "2. \n",
    "Dla nowych użytkowników można stosować globalne rekomendacje, żeby proponować popularne gry. Ponieważ mamy mnóstwo gier, a każdy użytkownik zagra w małą cześć, wykorzystałbym podejście oparte o rozkład macierzy, dzięki czemu dwójka użytkowników nie musi grać dokładnie w te same gry, tylko wystarczy, że będą podobne.\n",
    "3. Jako, że mamy mnóstwo danych o ludziach, warto używać personalizowanego algorytmu, ponieważ ludzie lubią być traktowani personalnie, nie każdy chce wiadomości z pudelka.\n",
    "- Content-based, ponieważ z natury newsów, bardziej nas interesuje jaka jest tematyka newsa, niż dokładnie jaki news. Często ludzie są zamknięci w `bańkach` i ograniczają się do kilku tematów.\n",
    "- Implicit-feedback, ponieważ dane są zbierane przez kliknięcia, długość przebywania na news. Mogą one być zakłócane, więc nie są precyzyjne. Ponadto są nieograniczone z góry.\n",
    "4. Możliwe, że nie została wzięta pod uwagę liczba ocen, tylko średnia. Wtedy można skorzystać z modelu Bayesowskiego\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waxvXlg96dn-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Źródła inspiracji do zadań dla zainteresowanych:\n",
    "1. Praca autora tego zestawu.\n",
    "2. Steam API oraz SteamSpy pozwalają łatwo zbierać duże zbiory danych z tej platformy, powstało w ten sposób zresztą całkiem sporo projektów, prac dyplomowych i artykułów naukowych (np. [projekt 1](http://brandonlin.com/steam.pdf), [projekt 2](https://library.ucsd.edu/dc/object/bb5021836n/_3_1.pdf), [praca dyplomowa](https://openaccess.mef.edu.tr/bitstream/handle/20.500.11779/1721/Serhan%20Bayram.pdf?sequence=1&isAllowed=y), [artykuł naukowy 1](https://www.researchgate.net/publication/333072035_Recommender_Systems_for_Online_Video_Game_Platforms_the_Case_of_STEAM), [artykuł naukowy 2](https://trepo.tuni.fi/bitstream/handle/10024/122499/a_hybrid_recommender_system_2020.pdf;jsessionid=E796B8E915FBBF37EF1E0B75210D8690?sequence=2)). Przykładowe zbiory: [dataset 1](https://www.kaggle.com/datasets/nikdavis/steam-store-games), [dataset 2](https://www.kaggle.com/datasets/forgemaster/steam-reviews-dataset), [dataset 3](https://cseweb.ucsd.edu/~jmcauley/datasets.html#steam_data).\n",
    "3. Luźno wzorowane na [zbiorze danych CI&T Deskdrop](https://www.kaggle.com/datasets/gspmoreira/articles-sharing-reading-from-cit-deskdrop).\n",
    "4. Artykuł [\"How not to sort by average rating\"](https://www.evanmiller.org/how-not-to-sort-by-average-rating.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pWYCSVB6dn-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Zadanie dla chętnych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pMXE4Hj6dn-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Uruchom na zbiorze MovieLens-1M (albo innym podobnego rozmiaru) algorytm LightGCN ([artykuł](https://arxiv.org/pdf/2002.02126.pdf)), implementujący podejście grafowe do rekomendacji, z użyciem biblioteki LibRecommender ([tutorial](https://github.com/massquantity/LibRecommender/blob/master/examples/pure_ranking_example.py)), która pod spodem używa PyTorch Geometric ([tutorial dla odważnych](https://colab.research.google.com/drive/1VfP6JlWbX_AJnx88yN1tM3BYE6XAADiy?usp=sharing)). Poniżej opis, jak to działa, ale znajomość teorii nie jest potrzebna do wykonania tego zadania :) Możesz użyć domyślnych hiperparametrów architektury sieci z tutoriala, ale zaimplementuj tuning przynajmniej liczby epok (LibRecommender nie ma early stoppingu). Jeżeli zbiór 1M jest za duży dla twojego sprzętu, możesz pracować na 100k. Jeżeli użyjesz MovieLens-1M, dla porównania zaimplementuj także wybrane 1-2 algorytmy z tego laboratorium na tym zbiorze (możesz po prostu skopiować kod z notebooka powyżej).\n",
    "\n",
    "W tym podejściu reprezentujemy problem jako graf, a nie jako macierz. Mamy graf dwudzielny użytkowników i przedmiotów, gdzie ocena reprezentowana jest jako krawędź między wierzchołkiem użytkownika a wierzchołkiem przedmiotu, opisana oceną. Rekomendacja polega na zadaniu **przewidywania krawędzi (edge prediction)**, czyli zasugerowanie dodania nowej krawędzi między użytkownikiem a przedmiotem.\n",
    "\n",
    "Sieć LightGCN implementuje podejście collaborative filtering na grafie. Jest to **grafowa sieć neuronowa (Graph Neural Network, GNN)**, osiągająca obecnie jedne z najlepszych wyników wśród systemów CF. Każdy wierzchołek ma tutaj wektor o pewnej założonej z góry długości $N$, tzw. embedding. Tworzy się go następująco:\n",
    "- robimy one-hot encoding dla użytkowników i przedmiotów, kodując ich `user_id` i `item_id`\n",
    "- mnożymy użytkowników przez macierz, robiąc kombinację liniową i rzutując na niższy wymiar\n",
    "- to samo, co wyżej, tylko dla przedmiotów\n",
    "Macierze embeddujące dla użytkowników i przedmiotów są parametrami, których uczymy się wraz z treningiem sieci neuronowej. Inicjalizuje się je losowo.\n",
    "\n",
    "Sieć LightGCN składa się z kilku warstw **konwolucji grafowej (graph convolution)**, gdzie każda warstwa agreguje informację z sąsiednich wierzchołków. Dla każdego wierzchołka robimy po prostu sumę ważoną wektorów sąsiadów ($e_u$ - embedding użytkownika $u$, $e_i$ - embedding przedmiotu $i$):\n",
    "$$\\large\n",
    "e_u^{(k+1)} = \\sum_{i \\in N(u)} \\frac{1}{\\sqrt{N(u)}\\sqrt{N(i)}} e_i^{(k)}\n",
    "$$\n",
    "Sąsiadów ważymy ich stopniem, aby wziąć pod uwagę popularność poszczególnych przedmiotów i aktywność użytkowników (mają duży stopień). Taka wymiana informacji między wierzchołkami propaguje informację w grafie, aktualizując embeddingi.\n",
    "\n",
    "Typowo takich warstw jest kilka, np. 3-4. Później agreguje się informację ze wszystkich warstw, w odróżnieniu od sieci CNN dla obrazów, gdzie zwykle bierze się wyjście tylko z ostatniej warstwy. Dla każdego użytkownika (i przedmiotu) bierzemy jego embedding z każdej warstwy i uśredniamy je. Daje to bogatą reprezentację wierzchołka i agreguje informacje zarówno z bliskiego sąsiedztwa (głębokie warstwy), jak i z ogółu społecznści w grafie (wysokie warstwy).\n",
    "\n",
    "Predykcja to po prostu iloczyn skalarny embeddingu użytkownika i przedmiotu: $r_{ij} = e_j^T e_i$. Sieć taką uczy się zwykle funkcją kosztu **Bayesian Personalized Ranking (BPR)**, używaną powszechnie w sieciach neuronowych do systemów rekomendacyjnych. Oczywiście uwzględnia się tu wszystkie typowe elementy sieci neuronowych: learning rate, weight decay etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9BELX9B6dn-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "763px",
    "left": "10px",
    "top": "150px",
    "width": "214.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
